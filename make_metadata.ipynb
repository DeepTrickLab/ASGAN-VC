{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate speaker embeddings and metadata for training\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "#from factory.LstmDV import LstmDV\n",
    "from factory.MetaDV import MetaDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many speaker you train for embedding\n",
    "C = MetaDV(80).to(device)\n",
    "C.load_state_dict(torch.load(\"model/static/metadv_vctk80.pth\", map_location=device))\n",
    "#C = LstmDV(80).to(device)\n",
    "#C.load_state_dict(torch.load(\"../model/static/dv_vctk80.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW MANY DIFFERENT CONTENT OF VOICE IN YOUR DATA\n",
    "num_uttrs = 3 #200\n",
    "errorment = 1 #16\n",
    "len_crop = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory: ./spmel\n"
     ]
    }
   ],
   "source": [
    "# Directory containing mel-spectrograms, modify your own\n",
    "rootDir = './spmel'\n",
    "dirName, subdirList, _ = next(os.walk(rootDir))\n",
    "print('Found directory: %s' % dirName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_along_axis(array: np.ndarray, target_length: int, axis: int = 0):\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "    if pad_size <= 0:\n",
    "        return array\n",
    "    npad = [(0, 0)] * array.ndim\n",
    "    npad[axis] = (0, pad_size)\n",
    "    return np.pad(array, pad_width=npad, mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing speaker: p225\n",
      "Processing speaker: p226\n",
      "Processing speaker: p227\n"
     ]
    }
   ],
   "source": [
    "speakers = []\n",
    "for speaker in sorted(subdirList):\n",
    "    print('Processing speaker: %s' % speaker)\n",
    "    utterances,embs  = [],[]\n",
    "    utterances.append(speaker)\n",
    "    _, _, fileList = next(os.walk(os.path.join(dirName,speaker)))\n",
    "    # make speaker embedding\n",
    "    assert len(fileList) >= num_uttrs\n",
    "    idx_uttrs = np.random.choice(len(fileList), size=num_uttrs, replace=False)\n",
    "    for i in range(errorment):\n",
    "        tmp = np.load(os.path.join(dirName, speaker, fileList[idx_uttrs[i]]))\n",
    "        # pad if the current one is too short   \n",
    "        if tmp.shape[0] < len_crop:\n",
    "            tmp = pad_along_axis(tmp,len_crop)\n",
    "            melsp = torch.from_numpy(tmp[np.newaxis,:, :]).cuda()\n",
    "        else:\n",
    "            melsp = torch.from_numpy(tmp[np.newaxis,:, :]).cuda()\n",
    "        emb = C(melsp)[1]\n",
    "        embs.append(emb.detach().squeeze().cpu().numpy())    \n",
    "           \n",
    "    utterances.append(np.mean(embs, axis=0))\n",
    "    # create file list\n",
    "    want_uttr = 0\n",
    "    for fileName in sorted(fileList):\n",
    "        tmp = np.load(os.path.join(dirName, speaker,fileName))\n",
    "        # 不足 100 的丟掉\n",
    "        if tmp.shape[0] < 100:\n",
    "            print(tmp.shape)\n",
    "            continue\n",
    "        elif want_uttr < num_uttrs:\n",
    "            utterances.append(os.path.join(speaker,fileName))\n",
    "            want_uttr += 1\n",
    "        else:\n",
    "            continue\n",
    "    speakers.append(utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(rootDir, 'train.pkl'), 'wb') as handle:\n",
    "    pickle.dump(speakers, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
